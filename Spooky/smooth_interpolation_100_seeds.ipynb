{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7380393b-d841-4e49-90a6-5b5406274054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy for arrays\n",
    "import torch  # PyTorch for tensors\n",
    "import torch.nn as nn  # Neural network module\n",
    "from torch.utils.data import DataLoader, TensorDataset  # Data loading utilities\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "from matplotlib import cm  # Colormap for plots\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6162c-0077-44ef-ade9-0e01c1d9d664",
   "metadata": {},
   "source": [
    "# Data Generating Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7ebe10-5d87-4cbe-995c-98fd9635fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp_y(x):\n",
    "    torch.manual_seed(123)\n",
    "    y = 2*(1-torch.exp(-torch.abs(x+torch.sin(x**2))))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69304e5a-66d7-43a3-a284-536730f14439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.arange(0,6,0.5).unsqueeze(dim=1)\n",
    "y_train = dgp_y(x_train)\n",
    "x_test = torch.arange(0,6,0.01).unsqueeze(dim=1)\n",
    "y_test = dgp_y(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697111e1-99da-4f99-8275-a604ecd8628e",
   "metadata": {},
   "source": [
    "## Saving the data for plotting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd81127-7ecb-41c3-a151-8c37ae6231d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df = pd.DataFrame({'X_train': x_train.numpy().flatten() , 'Y_train': y_train.numpy().flatten()})\n",
    "\n",
    "results_df = pd.DataFrame({'X_test': x_test.numpy().flatten()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc72d6a-7f9a-4115-8638-8b22f568fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(x_train,y_train, color = \"r\")\n",
    "#plt.plot(x_test,y_test)\n",
    "#plt.title(\"Data Generating Process\")\n",
    "#plt.xlabel(\"x\")\n",
    "#plt.ylabel(\"y\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad74378-9f55-422c-afcc-f6a2e0a8d3d4",
   "metadata": {},
   "source": [
    "# Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd5c6e-752a-4885-bf54-ae9e5c03b6e6",
   "metadata": {},
   "source": [
    "## Setting up the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a02460c-f19c-4696-ac01-eb86f2784df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_hidden = 128):\n",
    "        super().__init__()\n",
    "        self.dim_hidden= dim_hidden\n",
    "        \n",
    "        self.y = nn.Sequential(\n",
    "            nn.Linear(1, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.y(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d961ad7-e13b-4451-919c-4224ea96c5b3",
   "metadata": {},
   "source": [
    "## setting up the data loader (outside of the training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b0f019-4564-41ff-984b-d84d27e3e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = x_train.shape[0] # We are going full batch\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016aa52d-9d60-4777-9fab-bd0f640b9432",
   "metadata": {},
   "source": [
    "## Setting up the optimizer \n",
    "You might ask why I switched from `LBFGS` to `Adam`. The previous exercise, found in `smooth_interpolation`, focused on finding the interpolation threshold, a task where `LBFGS` excels. However, here we want to illustrate a point about the standard practices in deep learning. In practice, most of the community uses `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b5bb8d-efaa-4d12-b6a8-db2e07a09331",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "def training():\n",
    "    model = NN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "    num_epochs = 10000\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x_batch)\n",
    "            loss = criterion(y_hat, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % 5000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.5f}')\n",
    "    return model(x_test).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22249a72-4178-4acd-a123-1dac87a9c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch [5000/10000], Loss: 0.00002\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "1\n",
      "Epoch [5000/10000], Loss: 0.00010\n",
      "Epoch [10000/10000], Loss: 0.00017\n",
      "2\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "3\n",
      "Epoch [5000/10000], Loss: 0.00004\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "4\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "5\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "6\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "7\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "8\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "9\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "10\n",
      "Epoch [5000/10000], Loss: 0.00025\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "11\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00003\n",
      "12\n",
      "Epoch [5000/10000], Loss: 0.00096\n",
      "Epoch [10000/10000], Loss: 0.00008\n",
      "13\n",
      "Epoch [5000/10000], Loss: 0.00003\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "14\n",
      "Epoch [5000/10000], Loss: 0.00048\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "15\n",
      "Epoch [5000/10000], Loss: 0.00012\n",
      "Epoch [10000/10000], Loss: 0.00014\n",
      "16\n",
      "Epoch [5000/10000], Loss: 0.00004\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "17\n",
      "Epoch [5000/10000], Loss: 0.00063\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "18\n",
      "Epoch [5000/10000], Loss: 0.00080\n",
      "Epoch [10000/10000], Loss: 0.00011\n",
      "19\n",
      "Epoch [5000/10000], Loss: 0.00080\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "20\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00060\n",
      "21\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "22\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00222\n",
      "23\n",
      "Epoch [5000/10000], Loss: 0.00005\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "24\n",
      "Epoch [5000/10000], Loss: 0.00016\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "25\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "26\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "27\n",
      "Epoch [5000/10000], Loss: 0.00006\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "28\n",
      "Epoch [5000/10000], Loss: 0.00009\n",
      "Epoch [10000/10000], Loss: 0.00012\n",
      "29\n",
      "Epoch [5000/10000], Loss: 0.00079\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "30\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "31\n",
      "Epoch [5000/10000], Loss: 0.00010\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "32\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "33\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "34\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00004\n",
      "35\n",
      "Epoch [5000/10000], Loss: 0.00013\n",
      "Epoch [10000/10000], Loss: 0.00072\n",
      "36\n",
      "Epoch [5000/10000], Loss: 0.00150\n",
      "Epoch [10000/10000], Loss: 0.00042\n",
      "37\n",
      "Epoch [5000/10000], Loss: 0.00003\n",
      "Epoch [10000/10000], Loss: 0.00004\n",
      "38\n",
      "Epoch [5000/10000], Loss: 0.00002\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "39\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00005\n",
      "40\n",
      "Epoch [5000/10000], Loss: 0.00002\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "41\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00006\n",
      "42\n",
      "Epoch [5000/10000], Loss: 0.00015\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "43\n",
      "Epoch [5000/10000], Loss: 0.00009\n",
      "Epoch [10000/10000], Loss: 0.00003\n",
      "44\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "45\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "46\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "47\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "48\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "49\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00030\n",
      "50\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00007\n",
      "51\n",
      "Epoch [5000/10000], Loss: 0.00035\n",
      "Epoch [10000/10000], Loss: 0.00002\n",
      "52\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "53\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00008\n",
      "54\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "55\n",
      "Epoch [5000/10000], Loss: 0.00017\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "56\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "57\n",
      "Epoch [5000/10000], Loss: 0.00008\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "58\n",
      "Epoch [5000/10000], Loss: 0.00028\n",
      "Epoch [10000/10000], Loss: 0.00018\n",
      "59\n",
      "Epoch [5000/10000], Loss: 0.00012\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "60\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "61\n",
      "Epoch [5000/10000], Loss: 0.00002\n",
      "Epoch [10000/10000], Loss: 0.00003\n",
      "62\n",
      "Epoch [5000/10000], Loss: 0.00046\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "63\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00004\n",
      "64\n",
      "Epoch [5000/10000], Loss: 0.00005\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "65\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00007\n",
      "66\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "67\n",
      "Epoch [5000/10000], Loss: 0.00012\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "68\n",
      "Epoch [5000/10000], Loss: 0.00004\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "69\n",
      "Epoch [5000/10000], Loss: 0.00005\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "70\n",
      "Epoch [5000/10000], Loss: 0.00006\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "71\n",
      "Epoch [5000/10000], Loss: 0.00018\n",
      "Epoch [10000/10000], Loss: 0.00287\n",
      "72\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00025\n",
      "73\n",
      "Epoch [5000/10000], Loss: 0.00010\n",
      "Epoch [10000/10000], Loss: 0.00002\n",
      "74\n",
      "Epoch [5000/10000], Loss: 0.00238\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "75\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00006\n",
      "76\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "77\n",
      "Epoch [5000/10000], Loss: 0.00008\n",
      "Epoch [10000/10000], Loss: 0.00002\n",
      "78\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00006\n",
      "79\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "80\n",
      "Epoch [5000/10000], Loss: 0.00009\n",
      "Epoch [10000/10000], Loss: 0.00002\n",
      "81\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "82\n",
      "Epoch [5000/10000], Loss: 0.00002\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "83\n",
      "Epoch [5000/10000], Loss: 0.00005\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "84\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00002\n",
      "85\n",
      "Epoch [5000/10000], Loss: 0.00003\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "86\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00006\n",
      "87\n",
      "Epoch [5000/10000], Loss: 0.00009\n",
      "Epoch [10000/10000], Loss: 0.00004\n",
      "88\n",
      "Epoch [5000/10000], Loss: 0.00015\n",
      "Epoch [10000/10000], Loss: 0.00004\n",
      "89\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00003\n",
      "90\n",
      "Epoch [5000/10000], Loss: 0.00020\n",
      "Epoch [10000/10000], Loss: 0.00016\n",
      "91\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "92\n",
      "Epoch [5000/10000], Loss: 0.00011\n",
      "Epoch [10000/10000], Loss: 0.00008\n",
      "93\n",
      "Epoch [5000/10000], Loss: 0.00001\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "94\n",
      "Epoch [5000/10000], Loss: 0.00051\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "95\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00002\n",
      "96\n",
      "Epoch [5000/10000], Loss: 0.00007\n",
      "Epoch [10000/10000], Loss: 0.00001\n",
      "97\n",
      "Epoch [5000/10000], Loss: 0.00027\n",
      "Epoch [10000/10000], Loss: 0.00004\n",
      "98\n",
      "Epoch [5000/10000], Loss: 0.00039\n",
      "Epoch [10000/10000], Loss: 0.00000\n",
      "99\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [10000/10000], Loss: 0.00000\n"
     ]
    }
   ],
   "source": [
    "new_rows = []\n",
    "num_experiments = 100\n",
    "for i in range(num_experiments):\n",
    "    print(i)\n",
    "    y_hat = training().numpy().flatten()\n",
    "    column_name = f'y_hat_{i}'\n",
    "    new_rows = pd.DataFrame({column_name: y_hat})\n",
    "    results_df = pd.concat([results_df, new_rows], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9d05b6-576a-4dd0-9cee-2a1cea604827",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"./files/smooth_interpolation_100_seeds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0e9bfd-cda8-4635-b1c3-a4e80058cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.to_csv(\"./files/smooth_interpolation_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc058f6-e75e-46c5-99ab-38035b232429",
   "metadata": {},
   "source": [
    "# counting the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489b4009-5c86-4fc7-b6ac-8a3e6077e257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33409"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the number of trainable weights and biases in the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The PyTorch model\n",
    "\n",
    "    Returns:\n",
    "    - total_params: The total number of parameters (weights + biases)\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()  # numel() returns the total number of elements (weights/biases) in the parameter\n",
    "    return total_params\n",
    "\n",
    "model = NN()\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c7922-4477-4595-adb1-999b0162a841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
